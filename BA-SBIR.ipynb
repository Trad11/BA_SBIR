{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE_TRAIN_LOADER = 32\n",
    "BATCH_SIZE_TEST_LOADER = 4\n",
    "TRAIN_EPOCHS = 3\n",
    "INPUT_RESIZE = 224\n",
    "FEATURE_VEC_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "#DATA './Data/Sketchy/256x256' 12500 Images\n",
    "DATA = './Data/Sketchy/256x256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img.to(\"cpu\")\n",
    "    img = torchvision.utils.make_grid(img)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare two paths, if both paths have same string until \"-\" then sketch matches image\n",
    "#Argument order IS RELEVANT!\n",
    "def is_matching(path_image, path_sketch):\n",
    "    splitted_image = path_image.split('.')[0]\n",
    "    splitted_sketch = path_sketch.split('-')[0]\n",
    "    if splitted_image==splitted_sketch:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "path_image = 'n07679356_23722.jpg'\n",
    "path_sketch = 'n07679356_23722-1.png'\n",
    "\n",
    "is_matching(path_image, path_sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImgSketchAtIndex(dataset,index):\n",
    "    print(f\"Path: {dataset[index][2]}\")\n",
    "    imshow(dataset[index][0])\n",
    "    print(f\"Path: {dataset[index][3]}\")\n",
    "    imshow(dataset[index][1])\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "def show_pairs(dataset, start, stop):\n",
    "    for i in range (start, stop):\n",
    "        showImgSketchAtIndex(dataset,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Programme\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     torchvision.transforms.Resize((INPUT_RESIZE,INPUT_RESIZE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expects root dir with EXACTLY 2 dirs in it\n",
    "#                               ->Images(dir name irrelevant)\n",
    "#                               ->Sketches(dir name irrelevant)\n",
    "# In THAT order\n",
    "class ImgSketchDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, train=True, trainsplit_decimal=0.9):\n",
    "        if not (trainsplit_decimal>0.0 and trainsplit_decimal<1.0):\n",
    "            raise ValueError(\"trainsplit_decimal must be in range ]0.0,1.0[\")\n",
    "\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        self.trainsplit_decimal = trainsplit_decimal\n",
    "        self.train = train\n",
    "        # imgs_dir e.g. QuickDraw_images_two\n",
    "        self.imgs_dirs = os.path.join(root, os.listdir(self.root)[0])\n",
    "        # sketches_dir e.g. QuickDraw_sketches_two\n",
    "        self.sketches_dirs = os.path.join(root, os.listdir(self.root)[1])\n",
    "        self.samples = self.make_samples(self.imgs_dirs, self.sketches_dirs, multiple_sketches_per_img=False)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        path_img, path_sketch = self.samples[index]\n",
    "        img = self.loader(path_img)\n",
    "        sketch = self.loader(path_sketch)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            sketch = self.target_transform(sketch)\n",
    "\n",
    "        return sketch, img, path_sketch, path_img\n",
    "\n",
    "    #return list of tuples with path to imgs,sketches like [(airplane1img.jpeg, airplane1sketch.jpeg),...]\n",
    "    #needs paths like './data/QuickDraw_images_two/airplane/image_00001.jpg' to load with loader   \n",
    "    #If more sketches than imgs => only #images pairs    \n",
    "    #Sketch and Image have to contain same folder structure (i.e. airplanes, alarmclocks... folders both in images and sketches folders)    \n",
    "    # Given trainsplit_decimal == 0.9 IF Trainset => first 90% Images, first 90% Sketches, IF Testset => last 10% images, last 10%sketches\n",
    "    #TODO Make only 1 sketch per 1 image pairs, no multiple sketches for each img -> Why? Because of ClipLoss (Use multiple_sketches_per_img=False)\n",
    "    def make_samples(self, imgs_dirs, sketches_dirs, multiple_sketches_per_img=False):\n",
    "        samples = []\n",
    "        # nth_loop = 0\n",
    "        for dir in os.listdir(imgs_dirs):\n",
    "            #./data/QuickDraw_images_two/airplane\n",
    "            path_to_imgs = os.path.join(imgs_dirs, dir)\n",
    "            #./data/QuickDraw_sketches_two/airplane\n",
    "            path_to_sketches = os.path.join(sketches_dirs, dir)\n",
    "\n",
    "            imgs = os.listdir(path_to_imgs)\n",
    "            sketches = os.listdir(path_to_sketches)\n",
    "            dataset_size_imgs = int(self.trainsplit_decimal*len(imgs))\n",
    "            if self.train == True:\n",
    "                imgs = imgs[:dataset_size_imgs]\n",
    "                # sketches = sketches[:dataset_size_imgs]\n",
    "            else:\n",
    "                imgs = imgs[dataset_size_imgs:]\n",
    "                #Cut off not needed sketches\n",
    "                sketches = sketches[dataset_size_imgs:]\n",
    "\n",
    "            #makes (img,sketch) pairs in form [(img1,sketch1-1),(img1,sketch1-2),(img2,sketch2-1),...,(img4,sketch4-3)]\n",
    "            if multiple_sketches_per_img == True:\n",
    "                index = 0\n",
    "                for img in imgs:\n",
    "                    at_least_one_match = False\n",
    "                    while(index<len(sketches)):\n",
    "                        if is_matching(img,sketches[index]):\n",
    "                            imgPath = os.path.join(path_to_imgs,img)\n",
    "                            sketchPath = os.path.join(path_to_sketches,sketches[index])\n",
    "                            samples.append((imgPath,sketchPath))\n",
    "                            at_least_one_match = True\n",
    "                            index+=1\n",
    "                        elif at_least_one_match == False:\n",
    "                            index+=1\n",
    "                        else:\n",
    "                            break\n",
    "            #alternatively makes (img,sketch) pairs in form [(img1,sketch1-1), (img2,sketch2-1),...,(img4,sketch4-1)]            \n",
    "            else:\n",
    "                index = 0\n",
    "                for img in imgs:\n",
    "                    while(index<len(sketches)):\n",
    "                        if is_matching(img,sketches[index]):\n",
    "                            imgPath = os.path.join(path_to_imgs,img)\n",
    "                            sketchPath = os.path.join(path_to_sketches,sketches[index])\n",
    "                            samples.append((imgPath,sketchPath))\n",
    "                            index+=1\n",
    "                            break\n",
    "                        else:\n",
    "                            index+=1\n",
    "\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = './Data/Sketchy/256x256_less'\n",
    "root = DATA\n",
    "train_set = ImgSketchDataset(root, transform, transform, train=True, trainsplit_decimal=0.9)\n",
    "test_set =  ImgSketchDataset(root, transform, transform, train=False, trainsplit_decimal=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#TODO Trainset batch size? Not sure what best size is, but for CLIPLOSS it should be pretty big\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE_TRAIN_LOADER, shuffle=True)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE_TEST_LOADER, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "#TODO fc3 output size?\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        #for 32x32 inputs\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\n",
    "        #for 64x64 inputs\n",
    "        #self.fc1 = nn.Linear(2704, 120)\n",
    "        \n",
    "        #for 128x128 inputs\n",
    "        #self.fc1 = nn.Linear(13456, 120)\n",
    "        \n",
    "        #for 224x224 inputs\n",
    "        self.fc1 = nn.Linear(44944, 120)\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, FEATURE_VEC_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "class resNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(pretrained=True)\n",
    "       #self.model = nn.Sequential(*list(self.model.modules())[:-1]) # strips off last linear layer\n",
    "        self.model = nn.Sequential(*(list(self.model.children())[:-1]))\n",
    "        self.fc1 = nn.Linear(2048, FEATURE_VEC_SIZE)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "       # print(x.shape)\n",
    "        x = self.model(x)\n",
    "        x = torch.squeeze(x)\n",
    "       # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "net = resNet()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipLoss(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            local_loss=False,\n",
    "            gather_with_grad=False,\n",
    "            cache_labels=False,\n",
    "            rank=0,\n",
    "            world_size=1,\n",
    "            use_horovod=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local_loss = local_loss\n",
    "        self.gather_with_grad = gather_with_grad\n",
    "        self.cache_labels = cache_labels\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.use_horovod = use_horovod\n",
    "\n",
    "        # cache state\n",
    "        self.prev_num_logits = 0\n",
    "        self.labels = {}\n",
    "\n",
    "#What is logit_scale?\n",
    "    def forward(self, image_features, text_features, logit_scale=1.0):\n",
    "        device = image_features.device\n",
    "\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        logits_per_text = logit_scale * text_features @ image_features.T\n",
    "\n",
    "        # calculated ground-truth and cache if enabled\n",
    "        num_logits = logits_per_image.shape[0]\n",
    "        if self.prev_num_logits != num_logits or device not in self.labels:\n",
    "            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "            if self.cache_labels:\n",
    "                self.labels[device] = labels\n",
    "                self.prev_num_logits = num_logits\n",
    "        else:\n",
    "            labels = self.labels[device]\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels) +\n",
    "            F.cross_entropy(logits_per_text, labels)\n",
    "            ) / 2\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#Train the model \n",
    "#For each epoch trains all data in trainloader and then validates on all testloader data\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "clip_loss = ClipLoss()\n",
    "def train(EPOCHS=5):\n",
    "    writer = SummaryWriter()\n",
    "    net.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        #Training loop\n",
    "        train_loss = 0\n",
    "        train_iter = 0\n",
    "        train_epoch_cosSim_mean = 0\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        for i, batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            sketches = batch[0].to(device)\n",
    "            images = batch[1].to(device)\n",
    "            sketch_feature_vecs = net(sketches)\n",
    "            img_feature_vecs = net(images)\n",
    "    \n",
    "            #print(sketch_feature_vecs)\n",
    "            #print(sketch_feature_vecs.shape)\n",
    "\n",
    "            #loss = torch.mean(torch.cdist(sketch_feature_vecs, img_feature_vecs))\n",
    "            loss = clip_loss.forward(sketch_feature_vecs, img_feature_vecs)\n",
    "            train_loss += loss\n",
    "            train_iter += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loop.set_description(f\"Epoch [{epoch}/{EPOCHS}]\")\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "\n",
    "            # print(loss.item())\n",
    "            cos = nn.CosineSimilarity(dim=1)\n",
    "            similarity = cos(sketch_feature_vecs, img_feature_vecs)\n",
    "            mean_cosSim_ofBatch = torch.mean(similarity)\n",
    "            if i % 20 == 0:\n",
    "                print(f\"Training Mean Cosine Similarity of Batch #{i} is {mean_cosSim_ofBatch}]\")\n",
    "\n",
    "            train_epoch_cosSim_mean += mean_cosSim_ofBatch\n",
    "        train_epoch_cosSim_mean = train_epoch_cosSim_mean/train_iter\n",
    "        train_epoch_loss = train_loss/train_iter    \n",
    "        print(f\"Training Mean Cosine Similarity of EPOCH#{epoch} is [{train_epoch_cosSim_mean}]\")    \n",
    "        print(f\"Training Mean Loss of EPOCH#{epoch} is [{train_epoch_loss}]\")\n",
    "        # print(loss.item())\n",
    "        writer.add_scalar(\"CosineSim/Train\", train_epoch_cosSim_mean, epoch)\n",
    "        writer.add_scalar(\"Loss/Train\", train_epoch_loss, epoch)\n",
    "\n",
    "\n",
    "        #Validation Loop\n",
    "        loop = tqdm(enumerate(test_loader), total=len(test_loader), leave=False)\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        val_iter = 0\n",
    "        validation_epoch_cosSim_mean = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in loop:\n",
    "                sketches = batch[0].to(device)\n",
    "                images = batch[1].to(device)\n",
    "                sketch_feature_vecs = net(sketches)\n",
    "                img_feature_vecs = net(images)\n",
    "                \n",
    "\n",
    "                #print(sketch_feature_vecs)\n",
    "                #print(sketch_feature_vecs.shape)\n",
    "\n",
    "\n",
    "                #loss = torch.mean(torch.cdist(sketch_feature_vecs, img_feature_vecs))\n",
    "                loss = clip_loss.forward(sketch_feature_vecs, img_feature_vecs)\n",
    "                val_loss += loss\n",
    "                val_iter += 1\n",
    "                loop.set_description(f\"Validation Epoch [{epoch}/{EPOCHS}]\")\n",
    "                loop.set_postfix(loss = loss.item())\n",
    "\n",
    "                #Cosinesim sketch_vec,img_vec\n",
    "                cos = nn.CosineSimilarity(dim=1)\n",
    "                similarity = cos(sketch_feature_vecs, img_feature_vecs)\n",
    "                mean_cosSim_ofBatch = torch.mean(similarity)\n",
    "                if i % 20 == 0:\n",
    "                    print(f\"Validation Mean Cosine Similarity of Batch #{i} is {mean_cosSim_ofBatch}]\")\n",
    "\n",
    "                validation_epoch_cosSim_mean += mean_cosSim_ofBatch\n",
    "\n",
    "        validation_epoch_cosSim_mean = validation_epoch_cosSim_mean/train_iter\n",
    "        validation_epoch_loss = val_loss/val_iter   \n",
    "        print(f\"Validation Mean Cosine Similarity of EPOCH#{epoch} is [{validation_epoch_cosSim_mean}]\")    \n",
    "        print(f\"Validation Mean Loss of EPOCH#{epoch} is [{validation_epoch_loss}]\")\n",
    "        writer.add_scalar(\"CosineSim/Validation\", validation_epoch_cosSim_mean, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", validation_epoch_loss, epoch)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close\n",
    "    PATH = './sketchy_net.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "INFERENCE PHASE FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resNet(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = net.to(device)\n",
    "PATH = './sketchy_net.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after training is done to save all img feature vecs from training+test datasets, to be able to retrieve them later in testing.\n",
    "# returns retrieval_imgs_db => [(img_feature_vec1, img1), (img_feature_vec2, img2),...]\n",
    "def save_all_imgs_to_retrieval_db():\n",
    "    retrieval_imgs_db = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            images = batch[1].to(device)\n",
    "            images_feature_vecs = net(images)\n",
    "            zipped = tuple(zip(images_feature_vecs, images))\n",
    "            for element in zipped:\n",
    "                retrieval_imgs_db.append(element)\n",
    "                \n",
    "                    \n",
    "        for batch in test_loader:\n",
    "            images = batch[1].to(device)\n",
    "            images_feature_vecs = net(images)\n",
    "            zipped = tuple(zip(images_feature_vecs, images))\n",
    "            for element in zipped:\n",
    "                retrieval_imgs_db.append(element)\n",
    "\n",
    "        assert torch.equal(images_feature_vecs[-2], retrieval_imgs_db[-2][0]) == True\n",
    "        assert torch.equal(images[-1], retrieval_imgs_db[-1][1]) == True\n",
    "\n",
    "        return retrieval_imgs_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_db = save_all_imgs_to_retrieval_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goes over whole batch\n",
    "#returns most similar images ranked list to corresponding sketches (n=batch size) list: [most_similar_imgs_list_to_sketch#1, most_similar_imgs_list_to_sketch#2, ... most_similar_imgs_list_to_sketch#n]\n",
    "def retrieve_imgs(sketch_feature_vecs, images_db):\n",
    "    list_of_images = []\n",
    "    for sketch_feature_vec in sketch_feature_vecs:\n",
    "        max_similarity = 0\n",
    "        images = []\n",
    "        for img_feature_vec, image in images_db:\n",
    "            cos = nn.CosineSimilarity(dim=0)\n",
    "            similarity = cos(sketch_feature_vec, img_feature_vec)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_image = image\n",
    "                images.append(max_image)\n",
    "                #print(max_similarity)\n",
    "        images.reverse()\n",
    "        list_of_images.append(images)        \n",
    "    return list_of_images   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs over data_loader and calculates how many images are found correctly in the imgs_db given input sketch\n",
    "def calc_model_accuracy(data_loader):\n",
    "    with torch.no_grad():\n",
    "        hits = 0\n",
    "        images_amount = len(data_loader.dataset)\n",
    "        for data in tqdm(data_loader):\n",
    "            sketches = data[0].to(device)\n",
    "            images = data[1].to(device)\n",
    "            sketch_vecs = net(sketches)\n",
    "            result_imgs = retrieve_imgs(sketch_vecs, imgs_db)\n",
    "\n",
    "            # print(\"no zipped sketch\")\n",
    "            # imshow(sketches[0])\n",
    "            # print(\"no zipped inpute image\")\n",
    "            # imshow(images[0])\n",
    "            # print(\"no zipped result images\")\n",
    "            # for rank, resultImage in enumerate(result_imgs[0],1):\n",
    "            #     print(\"Rank \",rank)\n",
    "            #     imshow(resultImage)\n",
    "            \n",
    "            inputSketch_inputImage_resultImages = tuple(zip(sketches, images, result_imgs))\n",
    "            for inputSketch, inputImage, resultImages in inputSketch_inputImage_resultImages:\n",
    "                # print(\"inputSketch\")\n",
    "                # imshow(inputSketch)\n",
    "                # print(\"inputImage\")\n",
    "                # imshow(inputImage)\n",
    "                # print(\"resultImages\")\n",
    "                for rank, resultImage in enumerate(resultImages,1):\n",
    "                    # print(\"Rank \",rank)\n",
    "                    # imshow(resultImage)\n",
    "                    if torch.equal(inputImage,resultImage) == True:\n",
    "                        hits += 1\n",
    "                        print(\"Hit! at rank \",rank)\n",
    "                        # print(\"Hit_Input_Sketch\")\n",
    "                        # imshow(inputSketch)\n",
    "                        # print(\"Hit_Input_Image\")\n",
    "                        # imshow(inputImage)\n",
    "                        # print(\"Hit_Result_Image\")\n",
    "                        # imshow(resultImage)\n",
    "                        break\n",
    "            \n",
    "        accuracy = hits/images_amount     \n",
    "        print(\"Total hits \",hits)  \n",
    "        print(\"Accuracy: \",accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 16/313 [02:24<44:45,  9.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Studium\\BA\\repos\\BA_SBIR\\BA-SBIR.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m calc_model_accuracy(test_loader)\n",
      "\u001b[1;32mg:\\Studium\\BA\\repos\\BA_SBIR\\BA-SBIR.ipynb Cell 20\u001b[0m in \u001b[0;36mcalc_model_accuracy\u001b[1;34m(data_loader)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m images \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sketch_vecs \u001b[39m=\u001b[39m net(sketches)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m result_imgs \u001b[39m=\u001b[39m retrieve_imgs(sketch_vecs, imgs_db)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# print(\"no zipped sketch\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# imshow(sketches[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(\"no zipped inpute image\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#     print(\"Rank \",rank)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     imshow(resultImage)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m inputSketch_inputImage_resultImages \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(sketches, images, result_imgs))\n",
      "\u001b[1;32mg:\\Studium\\BA\\repos\\BA_SBIR\\BA-SBIR.ipynb Cell 20\u001b[0m in \u001b[0;36mretrieve_imgs\u001b[1;34m(sketch_feature_vecs, images_db)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cos \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCosineSimilarity(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m similarity \u001b[39m=\u001b[39m cos(sketch_feature_vec, img_feature_vec)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39;49m max_similarity:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     max_similarity \u001b[39m=\u001b[39m similarity\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Studium/BA/repos/BA_SBIR/BA-SBIR.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     max_image \u001b[39m=\u001b[39m image\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calc_model_accuracy(test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1909db895ad64418fdb8dae0b4f38ed8545dbd2a166e0f045a39ed71236c7023"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
